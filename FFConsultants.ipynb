{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ccc9ca9",
   "metadata": {},
   "source": [
    "# Data: Fairfax County Fire and Rescue Department call data and patient demographics \n",
    "Team: Fairfax Consultants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c429f0b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Hello Fairfax Consultants, Welcome Aboard\")\n",
    "#pip install nbconvert #or conda install nbconvert  #to save the notebook as .pdf\n",
    "#nbconvert[webpdf]\n",
    "#pip install pyppeteer\n",
    "#https://github.com/jgm/pandoc/releases/tag/2.18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e169684",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Check the version\n",
    "!jupyter --version\n",
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3123de9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "#pd.set_option('display.max_rows', None)\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "#Import Viz libraries\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "#Altair can be installed, along with the example datasets in vega_datasets, using:$ pip install altair vega_datasets\n",
    "#https://altair-viz.github.io/gallery/index.html\n",
    "\n",
    "import altair as alt #pip install altair (from anaconda Powershell)\n",
    "from vega_datasets import data # pip install vega_datasets/ #Only to use data from vegas data sets or else can be ignored\n",
    "\n",
    "#To avoid MaxRowsError: in big data sets with altair\n",
    "alt.data_transformers.disable_max_rows()\n",
    "alt.data_transformers.enable('json')\n",
    "#pip install altair_data_server\n",
    "#alt.data_transformers.enable('data_server')\n",
    "# Optional in Jupyter Notebook: requires an up-to-date vega nbextension.\n",
    "#alt.renderers.enable('notebook') #$ pip install vega #https://altair-viz.github.io/user_guide/display_frontends.html\n",
    "\n",
    "#pip install altair_viewer\n",
    "#alt.renderers.enable('altair_viewer') \n",
    "#pip install altair_saver #to save interactive charts with interactive elements\n",
    "\n",
    "#alt.renderers.enable('svg') #renders the chart as a static svg image within a Jupyter notebook.\n",
    "#chart.save('chart.html', embed_options={'renderer':'svg'}) #o change to svg rendering, use the embed_options as such"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f4cbc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import all sheet by index\n",
    "# # Set current working directory\n",
    "# os.chdir(\"C:\\\\Users\\\\matth\\\\OneDrive\\\\Documents\\\\GMU\\\\DAEN 690\\\\Sponsor Files\")\n",
    "#df = pd.read_excel(\"E:\\DAEN_690\\Data\\RawData_GMU_Summer2022.xlsx\", sheet_name = None)#[0,1,2,3])\n",
    "\n",
    "df = pd.read_excel(\"E:\\DAEN_690\\Data\\RawData_GMU_Summer2022_SecondaryImpression.xlsx\", sheet_name = None)#[0,1,2,3])\n",
    "\n",
    "# # reading csv file and at a same time using converters attribute which will remove extra space\n",
    "# df = pd.read_csv('\\\\student_data.csv', converters={'Name': str.strip(),\n",
    "#                                                 'Blood Group' : str.strip(),\n",
    "#                                                 'Gender' : str.strip() } )\n",
    "\n",
    "#df = pd.read_excel(\"E:\\DAEN_690\\Data\\Copy of RawData_GMU_Summer2022_SecondaryImpression.xlsx\", sheet_name = None)#[0,1,2,3]) #in laptop\n",
    "#df = pd.read_excel(\"E:\\DAEN_690\\Data\\Copy of RawData_GMU_Summer2022.xlsx\", sheet_name = None)#[0,1,2,3]) #in laptop\n",
    "#pd.ExcelFile()\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d930b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imported as Dictionary, view keys or values\n",
    "print(df.keys())\n",
    "print(df.values())\n",
    "print(df['Patient Demographics']) #view "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7257aee6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#convert Dicationary to Data Frames\n",
    "PatDemo = pd.DataFrame(df['Patient Demographics'])\n",
    "SEPSIS = pd.DataFrame(df['sepsis'])\n",
    "STEMI = pd.DataFrame(df['STEMI'])\n",
    "STROKE = pd.DataFrame(df['stroke'])\n",
    "\n",
    "#View Headings\n",
    "print(\"\\n PatDemo:\\n\",PatDemo.head(1))\n",
    "print(\"\\n SEPSIS:\\n\",SEPSIS.head(1))\n",
    "print(\"\\n STEMI:\\n\",STEMI.head(1))\n",
    "print(\"\\n STROKE:\\n\",STROKE.head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62216b09",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#View Columns for each data frame\n",
    "\n",
    "print(\"\\n PatDemo:\\n\",PatDemo.columns)\n",
    "print(\"\\n SEPSIS:\\n\",SEPSIS.columns )\n",
    "print(\"\\n STEMI:\\n\",STEMI.columns )\n",
    "print(\"\\n STROKE:\\n\",STROKE.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680bd5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count Nulls\n",
    "print(PatDemo.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ffc321",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#With the help of heatmap, we can see the amount of data that is missing from the attribute\n",
    "sns.heatmap(PatDemo.isnull(),cbar=False,cmap='viridis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee19aa2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Check if any columns or values are duplicated.\n",
    "\n",
    "print(\"\\n If any column header redundant?:\",PatDemo.columns.duplicated())\n",
    "print(\"\\n Sex & Gender Duplicated?   : \",PatDemo['PatientSex'].equals(PatDemo['PatientGender']))\n",
    "print(\"\\n Race Duplicated?           : \",PatDemo['Patient_Race_List_Raw'].equals(PatDemo['PatientRace']))\n",
    "print(\"\\n Gender and GRaw Duplicated?: \",PatDemo['Patient_Gender_Raw'].equals(PatDemo['PatientGender']))\n",
    "\n",
    "#PatDemo.columns\n",
    "#even if logicaltest says not duplicated, it is okay to delete 'PatientSex',  'Patient_Gender_Raw', 'Patient_Race_List_Raw'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7940bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop reduntant columns\n",
    "PatDemo.drop([\"Patient_Race_List_Raw\", \"Patient_Gender_Raw\",'PatientGender'], axis =1,  inplace =True)\n",
    "PatDemo.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ab6779",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename patient demographic column 'Patient_ID_Internal' to 'PtIDInternal'\n",
    "PatDemo.rename(columns = {'Patient_ID_Internal':'PtIDInternal'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c9acf1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Count of unique values in each rows for each column\n",
    "  \n",
    "print(\"No.of.unique values in each column :\\n\", PatDemo.nunique(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b63c83",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#read unique values, how many patients under particular age group\n",
    "\n",
    "#print(\"\\n unique values: \", PatDemo['AgeGroup_HIPAA'].unique() )\n",
    "#print(\"\\n Count of unique values: \", len(PatDemo['AgeGroup_HIPAA'].unique()))\n",
    "#print(\"\\n Count of unique values: \", PatDemo.AgeGroup_HIPAA.nunique())\n",
    "#print(\"\\n Count of each unique values: \", list(PatDemo.AgeGroup_HIPAA.value_counts()))\n",
    "print(\"\\n Count of each unique values: \", PatDemo.groupby('AgeGroup_HIPAA').PtIDInternal.nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7999796",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert AgeGroup_HIPAA to string to get rid of datetime format value\n",
    "PatDemo['AgeGroup_HIPAA']= PatDemo['AgeGroup_HIPAA'].astype('str')\n",
    "#print(\"\\n sorted age group:\",sorted(PatDemo['AgeGroup_HIPAA'].unique()))\n",
    "print(\"\\n Count of each unique values: \", PatDemo.groupby('AgeGroup_HIPAA').PtIDInternal.nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d523abff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#check the data types for rest of the columns\n",
    "print(PatDemo.info())\n",
    "print(\"\\n details: \\n\",PatDemo['AgeGroup_HIPAA'].describe())\n",
    "# PatDemo.dtypes\n",
    "# PatDemo.index\n",
    "# PatDemo.shape\n",
    "# PatDemo.memory_usage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a274db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replace '2022-10-19 00:00:00' with 10-19 and nan with Unk, my logic is even NULL is also unknown in a way\n",
    "\n",
    "PatDemo[\"AgeGroup_HIPAA\"] = PatDemo[\"AgeGroup_HIPAA\"].replace({'2022-10-19 00:00:00':'10_19','20-29':'20_29','50-59':'50_59', \n",
    "                                                               '80-89':'80_89','70-79':'70_79','40-49':'40_49','Under 10':'<10', \n",
    "                                                               '60-69':'60_69','30-39':'30_39', 'nan':'Unk'})\n",
    "\n",
    "#sorted(PatDemo[\"AgeGroup_HIPAA\"].unique())\n",
    "print(\"\\n Count of each unique values: \", PatDemo.groupby('AgeGroup_HIPAA').PtIDInternal.nunique())\n",
    "# Total=sum(PatDemo['AgeGroup_HIPAA'].value_counts(dropna=True))\n",
    "# Total\n",
    "# PatDemo.AgeGroup_HIPAA.value_counts(dropna=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c5d83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Binarize the boolen values \n",
    "#PatDemo.columns\n",
    "#Get the boolean values for Patients Sex\n",
    "PatDemo = pd.get_dummies(PatDemo, columns=['PatientRace', 'PatientEthnicity', 'PatientSex','AgeGroup_HIPAA']) #, drop_first=True) #Since thers only two values male or female.\n",
    "PatDemo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6b2a31",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Lets drop redundant columns:\n",
    "PatDemo.drop(['PatientSex_Male','IsJuvenileUnder18','IsSenior65+'],axis =1,  inplace =True)\n",
    "#rename patient demographic column 'Patient_ID_Internal' to 'PtIDInternal'\n",
    "PatDemo.rename(columns = {'PatientSex_Female':'PatientSex_FM'}, inplace = True)\n",
    "PatDemo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2072e4a",
   "metadata": {},
   "source": [
    "#SEPSIS_Data Cleaning and Merge with Patients Demographics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769678b3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#View Header and count null\n",
    "\n",
    "print(\"\\n header:\\n\",SEPSIS.head(2))\n",
    "\n",
    "print(\"\\n count null before merge:\\n\", SEPSIS.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eabc8af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check if any columns or values are duplicated.\n",
    "#SEPSIS['EmergencyDepartmentDiagnosis'].unique()\n",
    "\n",
    "print(\"\\n If any column header redundant?:\",SEPSIS.columns.duplicated())\n",
    "\n",
    "print(\"\\n Emergency & Hospital have same values?   : \",SEPSIS['EmergencyDepartmentDiagnosis'].equals(SEPSIS['HospitalDiagnosis']))\n",
    "\n",
    "\n",
    "print(\"\\n Primary and Secondary Impression Duplicated?: \",SEPSIS['PrimaryImpression'].equals(SEPSIS['SecondaryImpression']))\n",
    "\n",
    "\n",
    "# print(\"\\n Race Duplicated?           : \",SEPSIS['Patient_Race_List_Raw'].equals(SEPSIS['PatientRace']))\n",
    "# print(\"\\n Gender and GRaw Duplicated?: \",SEPSIS['Patient_Gender_Raw'].equals(SEPSIS['PatientGender']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17325281",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## create a new column that is a combination of the emergency deparment diagnosis & hospital diagnosis\n",
    "# https://stackoverflow.com/questions/19377969/combine-two-columns-of-text-in-pandas-dataframe\n",
    "SEPSISclean = SEPSIS\n",
    "\n",
    "#SEPSISclean[\"overall_diagnosis\"] = SEPSISclean[\"EmergencyDepartmentDiagnosis\"].astype(str).fillna('') + SEPSISclean[\"HospitalDiagnosis\"].astype(str).fillna('')\n",
    "SEPSISclean[\"overall_diagnosis\"] = SEPSISclean[\"EmergencyDepartmentDiagnosis\"].fillna('') + SEPSISclean[\"HospitalDiagnosis\"].fillna('')\n",
    "\n",
    "SEPSISclean[\"Merged_Impression\"] = SEPSISclean[\"PrimaryImpression\"].fillna('') + SEPSISclean[\"SecondaryImpression\"].fillna('')\n",
    "\n",
    "##drop reduntant columns\n",
    "#SEPSIS.drop([\"EmergencyDepartmentDiagnosis\", \"HospitalDiagnosis\"], axis =1,  inplace =True)\n",
    "\n",
    "##View Header and count null\n",
    "\n",
    "print(\"\\n header:\\n\",SEPSISclean.head(2))\n",
    "\n",
    "print(\"\\n count null after merge:\\n\", SEPSISclean.isna().sum())\n",
    "\n",
    "#SEPSISclean.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117af888",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#SEPSISclean.dtypes\n",
    "print(\"\\n count NaN after merge:\\n\",SEPSISclean[SEPSISclean == ''].count())\n",
    "#SEPSISclean['overall_diagnosis'].eq('nannan').sum()##df1[['overall_diagnosis ', 'col3', 'col4']].eq(1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26632a4c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "SEPSISclean[\"overall_diagnosis\"].unique()\n",
    "\n",
    "# Infectious -Sepsis (A41.9)\n",
    "# \"Sepsis, unspecified organism (A41.9)\"\n",
    "#\"Other specified sepsis (A41.89)\"\n",
    "# \"Severe sepsis with septic shock (R65.21), \n",
    "# \"Severe sepsis without septic shock (R65.20)\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5fa467",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## create a new column that is 1 if the overall_diagnosis column contains the string \"A41.9\", 0 if not\n",
    "\n",
    "SEPSISclean[\"sepsis_outcome\"] = pd.np.where(SEPSISclean[\"overall_diagnosis\"].str.contains(\"Sepsis\", na=False, case=False), 1, \n",
    "                                #pd.np.where(SEPSISclean[\"overall_diagnosis\"].str.contains(\"sepsis\", na=False), \"1\",\n",
    "                                pd.np.where(SEPSISclean[\"overall_diagnosis\"].str.contains(\"A41.9\", na=False, case=False), 1, 0))#)\n",
    "# sepsis_demo[\"sepsis_outcome\"] = np.where(sepsis_demo[\"overall_diagnosis\"].str.contains(\"A41.9\", na=False, case=False), \"1\",\n",
    "#                                 np.where(sepsis_demo[\"overall_diagnosis\"].str.contains(\"A41.89\", na=False, case=False), \"1\", \"0\"))\n",
    "\n",
    "\n",
    "SEPSISclean.head()\n",
    "#binarize primary Impr\n",
    "SEPSISclean[\"ProviderImpression\"] = pd.np.where(SEPSISclean[\"Merged_Impression\"].str.contains(\"Sepsis\", na=False, case=False), 1, \n",
    "                               \n",
    "                             pd.np.where(SEPSISclean[\"Merged_Impression\"].str.contains(\"A41.9\", na=False, case=False), 1, 0))\n",
    "SEPSISclean.head()\n",
    "#Accuracy\n",
    "SEPSISclean[\"Accuracy\"] = pd.np.where(SEPSISclean[\"ProviderImpression\"] == SEPSISclean[\"sepsis_outcome\"], 1, 0) #Match -1, MisMatch 0\n",
    "SEPSISclean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c9dbf8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#To find out  if it is Under or Over Triage.\n",
    "#TRIAGE is the sorting of and allocation of treatment to patients\n",
    "\n",
    "SEPSISclean[\"Triage0\"] = np.where(((SEPSISclean[\"ProviderImpression\"] == SEPSISclean[\"sepsis_outcome\"])),'Triage','')\n",
    "SEPSISclean[\"Triage1\"] = np.where(((SEPSISclean[\"ProviderImpression\"] == 1) &  (SEPSISclean[\"sepsis_outcome\"] == 0)),'OverTriage','')\n",
    "SEPSISclean[\"Triage2\"] = np.where(((SEPSISclean[\"ProviderImpression\"] == 0) &  (SEPSISclean[\"sepsis_outcome\"] == 1)),'UnderTriage','')\n",
    "SEPSISclean[\"Triage\"] = SEPSISclean[\"Triage0\"].fillna('') + SEPSISclean[\"Triage1\"].fillna('') + SEPSISclean[\"Triage2\"].fillna('')\n",
    "\n",
    "print('Head: ',SEPSISclean.head())\n",
    "print('UniqueValue: ',SEPSISclean[\"Triage\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0545f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Check for unique values in each columns\n",
    "\n",
    "print(\"\\n Merged_Impressions Values: \\n\", SEPSISclean['Merged_Impression'].unique())\n",
    "print(\"\\n ProviderImpression Values: \\n\", SEPSISclean['ProviderImpression'].unique())\n",
    "print(\"\\n Unique qSOFA values: \\n\",SEPSISclean['qSOFA'].unique())\n",
    "print(\"\\n Unique Sepsis Statusvalues: \\n\",SEPSISclean['SepsisStatus'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5357ba03",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Crete dummies for SepsisStatus\n",
    "SEPSISclean = pd.get_dummies(SEPSISclean, columns=['SepsisStatus'])#, drop_first=True)\n",
    "SEPSISclean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e12e81a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#To iter through the values/ categories list nan are replaced with Unknown\n",
    "\n",
    "SEPSISclean[\"qSOFA\"] = SEPSISclean[\"qSOFA\"].fillna('Unknown')\n",
    "SEPSISclean['qSOFA'].unique()\n",
    "\n",
    "\n",
    "\n",
    "#Create dummies for qSOFA\n",
    "\n",
    "# SEPSISclean = pd.get_dummies(SEPSISclean, columns=['qSOFA'])#, drop_first=True) #will create 7 combinations, in fact we have only three criteria.\n",
    "# SEPSISclean\n",
    "#following method will identify three condition within the strings.\n",
    "\n",
    "dummies = SEPSISclean['qSOFA'].str.get_dummies(sep=', ') #(SEPSISclean[variable], prefix = variable)   # Get the dummy variables from pandas\n",
    "SEPSISclean = pd.concat([SEPSISclean,dummies],axis=1)       # concat dummy variables into original data \n",
    "#SEPSISclean.drop(SEPSISclean['qSOFA'],axis=1,inplace=True)      # inplace  = True : becasue we want to work on the dataframe directly \n",
    "SEPSISclean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978b556f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##Drop redundant column \n",
    "SEPSISclean.drop(['MonthYear','PrimaryImpression','SecondaryImpression',\"EmergencyDepartmentDiagnosis\",\"HospitalDiagnosis\",\n",
    "                  \"overall_diagnosis\",'qSOFA','Merged_Impression','Triage1','Triage2','Triage0','Unknown'],\n",
    "                 axis =1,  inplace =True)\n",
    "SEPSISclean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba382ba",
   "metadata": {},
   "source": [
    "Merge with Patients DemoGraphic Informations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a74072",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# merging the files how='inner' by default.\n",
    "PdmSEPSIS = pd.merge(PatDemo,SEPSISclean, on='PtIDInternal') \n",
    "print(PdmSEPSIS.head(3))\n",
    "\n",
    "#Rearrange the columns\n",
    "last_cols = ['Accuracy', 'Triage']\n",
    "first_cols = [col for col in PdmSEPSIS.columns if col not in last_cols]\n",
    "PdmSEPSIS = PdmSEPSIS[first_cols+last_cols]\n",
    "print(PdmSEPSIS.head(3))\n",
    "\n",
    "# #Write as .csv\n",
    "#PdmSEPSIS.to_excel(\"E:\\DAEN_690\\Data\\PdmSEPSIS.xlsx\",sheet_name='SEPSIS' , index=False)\n",
    "PdmSEPSIS.to_csv(\"E:\\DAEN_690\\Data\\PdmSEPSIS1.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1413efe1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Lets drop irrelavant  columns:\n",
    "PdmSEPSIS.drop(['PtIDInternal'],axis =1,  inplace =True)\n",
    "print(PdmSEPSIS.head(3))\n",
    "print(\"\\n info: \", PdmSEPSIS.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501aba3e",
   "metadata": {},
   "source": [
    "#STEMI_Data Cleaning and Merge with Patients Demographics\n",
    "Repeat the same process for STEMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52b5aa0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#View Header and count null\n",
    "\n",
    "print(\"\\n header:\\n\",STEMI.head(2))\n",
    "\n",
    "print(\"\\n count null before merge:\\n\", STEMI.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e0658c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check if any columns or values are duplicated.\n",
    "#STEMI['EmergencyDepartmentDiagnosis'].unique()\n",
    "print(\"\\n If any column header redundant?:\",STEMI.columns.duplicated())\n",
    "\n",
    "print(\"\\n Emergency & Hospital have same values?   : \",STEMI['EmergencyDepartmentDiagnosis'].equals(STEMI['HospitalDiagnosis']))\n",
    "\n",
    "print(\"\\n Primary and Secondary Impression Duplicated?: \",STEMI['PrimaryImpression'].equals(STEMI['SecondaryImpression']))\n",
    "\n",
    "# print(\"\\n Race Duplicated?           : \",STEMI['Patient_Race_List_Raw'].equals(STEMI['PatientRace']))\n",
    "# print(\"\\n Gender and GRaw Duplicated?: \",STEMI['Patient_Gender_Raw'].equals(STEMI['PatientGender']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6355db",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## create a new column that is a combination of the emergency deparment diagnosis & hospital diagnosis\n",
    "# https://stackoverflow.com/questions/19377969/combine-two-columns-of-text-in-pandas-dataframe\n",
    "STEMIclean = STEMI\n",
    "\n",
    "#STEMIclean[\"overall_diagnosis\"] = STEMIclean[\"EmergencyDepartmentDiagnosis\"].astype(str) + STEMIclean[\"HospitalDiagnosis\"].astype(str)\n",
    "STEMIclean[\"overall_diagnosis\"] = STEMIclean[\"EmergencyDepartmentDiagnosis\"].fillna('') + STEMIclean[\"HospitalDiagnosis\"].fillna('')\n",
    "STEMIclean[\"Merged_Impression\"] = STEMIclean[\"PrimaryImpression\"].fillna('') + STEMIclean[\"SecondaryImpression\"].fillna('')\n",
    "\n",
    "##drop reduntant columns\n",
    "#STEMI.drop([\"EmergencyDepartmentDiagnosis\", \"HospitalDiagnosis\"], axis =1,  inplace =True)\n",
    "\n",
    "##View Header and count null\n",
    "\n",
    "print(\"\\n header:\\n\",STEMIclean.head(2))\n",
    "\n",
    "print(\"\\n count null after merge:\\n\", STEMIclean.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81200ddf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#STEMIclean.dtypes\n",
    "#print(\"\\n count NaN after merge:\\n\",STEMIclean[STEMIclean == 'nannan'].count()) #975\n",
    "print(\"\\n count NaN after merge:\\n\",STEMIclean[STEMIclean == ''].count()) #975\n",
    "#STEMIclean['overall_diagnosis'].eq('nannan').sum()##df1[['overall_diagnosis ', 'col3', 'col4']].eq(1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f5ab46",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "STEMIclean.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ed3bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "STEMIclean[\"Merged_Impression\"].unique()\n",
    "\n",
    "# CV - Myocardial Infarction (Non-STEMI), With or Without Chest Pain (I21.4) are excluded\n",
    "# CV - STEMI of Anterior Wall, With or Without Chest Pain (I21.0)\n",
    "# CV - STEMI of Inferior Wall, With or Without Chest Pain (I21.1)\n",
    "# CV - STEMI of other sites, With or Without Chest Pain (I21.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6778dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## create a new column that is 1 if the overall_diagnosis column contains the string \"A41.9\", 0 if not\n",
    "#I21.0, I21.1, I21.2 #Codes used for STEMI\n",
    "STEMIclean[\"STEMI_outcome\"] = pd.np.where(STEMIclean[\"overall_diagnosis\"].str.contains(\"I21.0\", na=False, case=False), 1, 0)\n",
    "\n",
    "#pd.np.where(STEMIclean[\"overall_diagnosis\"].str.contains(\"(STEMI)\", na=False, case=False), \"1\", \n",
    "                                #pd.np.where(STEMIclean[\"overall_diagnosis\"].str.contains(\"STEMI\", na=False), \"1\",\n",
    "                                #)#)\n",
    "print(STEMIclean.head())\n",
    "#Write as .csv\n",
    "#STEMIclean.to_excel(\"E:\\DAEN_690\\Data\\PdmSTEMI.xlsx\",sheet_name='STEMI' , index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24f9f38",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## create a new column that is STEMI if the PrimaryImpression column contains the string \"STEMI\", Other if not\n",
    "STEMIclean[\"PrimaryImpr\"] = pd.np.where(STEMIclean[\"Merged_Impression\"].str.contains(\"CV - STEMI\", na=False, case=False), \"1\",\n",
    "                                        (pd.np.where(STEMIclean[\"Merged_Impression\"].str.contains(\"I21.\", na=False, case=False),\"1\",\"0\")))   \n",
    "                            \n",
    "                            #pd.np.where(STEMIclean[\"PrimaryImpression\"].str.contains(\"I21.0\" or \"I21.1\" or \"I21.2\", na=False, case=False), \"STEMI\", \"Other\")   \n",
    "                            \n",
    "print(STEMIclean.head())  \n",
    "\n",
    "#Write as .csv\n",
    "STEMIclean.to_excel(\"E:\\DAEN_690\\Data\\PdmSTEMI.xlsx\",sheet_name='STEMI' , index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063baf7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "STEMIclean['DCPSAMI'] = STEMIclean['DoesClinicalPicSuggestAcuteMI'].replace(['No','Uncertain','Yes'],['NO ClinicalPicSuggestAcuteMi','ClinicalPicSuggest Uncertain about AcuteMi','ClinicalPicSuggestAcuteMi'])\n",
    "    \n",
    "STEMIclean['DPIstSEl'] = STEMIclean['DoesProviderInterpretstSegmentElevation'].replace(['No','Uncertain','Yes'],['NO stSegmentElevation','Uncertain stSegmentElevation','stSegmentElevation'])\n",
    "    \n",
    "STEMIclean['DMSAMI'] = STEMIclean['DoesMonitorStateAcuteMI'].replace(['No','Uncertain','Yes'],['NO MonitorStateAcuteMi','MonitorState Uncertain about AcuteMi','MonitorStateAcuteMi',])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd558944",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge Questionnaire columns to easily analyze/ pivot\n",
    "STEMIclean[\"AcuteMI_stSegmentElevation\"] = STEMIclean['DCPSAMI'].fillna('') + STEMIclean['DPIstSEl'].fillna('') + STEMIclean['DMSAMI'].fillna('')\n",
    "#Write as .csv\n",
    "#STEMIclean.to_excel(\"E:\\DAEN_690\\Data\\PdmSTEMI.xlsx\",sheet_name='STEMI' , index=False)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f8be2a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"\\n PImpressions Values: \\n\", STEMIclean['PrimaryImpression'].unique())\n",
    "print(\"\\n PImpr Values: \\n\", STEMIclean['PrimaryImpr'].unique())\n",
    "#print(\"\\n Unique qSOFA values: \\n\",SEPSISclean['qSOFA'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48eb88c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "STEMIclean\n",
    "#Write as .xlsx\n",
    "STEMIclean.to_excel(\"E:\\DAEN_690\\Data\\STEMIclean.xlsx\",sheet_name='STEMI' , index=False)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0819ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "STEMIclean.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afafff4b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##Drop redundant column \n",
    "STEMIclean.drop(['MonthYear','PrimaryImpression','SecondaryImpression',\"EmergencyDepartmentDiagnosis\",\"HospitalDiagnosis\",\"overall_diagnosis\", 'DCPSAMI', 'DPIstSEl', 'DMSAMI'], axis =1,  inplace =True)\n",
    "STEMIclean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47836b0",
   "metadata": {},
   "source": [
    "Merge with Patient Demographic Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302b9143",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# merging the files how='inner' by default.\n",
    "PdmSTEMI = pd.merge(PatDemo,STEMIclean, on='PtIDInternal') \n",
    "print(PdmSTEMI.head())\n",
    "#Write as .csv\n",
    "PdmSTEMI.to_excel(\"E:\\DAEN_690\\Data\\PdmSTEMI.xlsx\",sheet_name='STEMI' , index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64be3f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(PdmSTEMI.head())\n",
    "print(PdmSTEMI.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21944756",
   "metadata": {},
   "outputs": [],
   "source": [
    "import altair as alt\n",
    "#from vega_datasets import data\n",
    "\n",
    "source = PdmSTEMI\n",
    "\n",
    "PatSTEMIChart = alt.Chart(source).mark_bar().encode(\n",
    "    x='AgeGroup_HIPAA:O',\n",
    "    y='count(AgeGroup_HIPAA):Q',\n",
    "    color='PrimaryImpr:N',\n",
    "    column='STEMIstatus:N',\n",
    "    tooltip=['PrimaryImpr', 'STEMI_outcome', 'AgeGroup_HIPAA:Q', 'count(AgeGroup_HIPAA)']\n",
    ").interactive()\n",
    "PatSTEMIChart .display()\n",
    "#PatDemChart.save('PatDemChart.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2806ace1",
   "metadata": {},
   "source": [
    "#STEMI_Data Cleaning and Merge with Patients Demographics\n",
    "\n",
    "REPEAT Process for STROKE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cbc4bb5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#View Header and count null\n",
    "\n",
    "print(\"\\n header:\\n\",STROKE.head(2))\n",
    "\n",
    "print(\"\\n count null before merge:\\n\", STROKE.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99847d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check if any columns or values are duplicated.\n",
    "#STROKE['EmergencyDepartmentDiagnosis'].unique()\n",
    "print(\"\\n If any column header redundant?:\",STROKE.columns.duplicated())\n",
    "\n",
    "print(\"\\n Emergency & Hospital have same values?   : \",STROKE['EmergencyDepartmentDiagnosis'].equals(STROKE['HospitalDiagnosis']))\n",
    "print(\"\\n Primary and Secondary Impression Duplicated?: \",STROKE['PrimaryImpression'].equals(STROKE['SecondaryImpression']))\n",
    "# print(\"\\n Race Duplicated?           : \",STROKE['Patient_Race_List_Raw'].equals(STROKE['PatientRace']))\n",
    "# print(\"\\n Gender and GRaw Duplicated?: \",STROKE['Patient_Gender_Raw'].equals(STROKE['PatientGender']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9caaf8c3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## create a new column that is a combination of the emergency deparment diagnosis & hospital diagnosis\n",
    "# https://stackoverflow.com/questions/19377969/combine-two-columns-of-text-in-pandas-dataframe\n",
    "STROKEclean = STROKE\n",
    "\n",
    "STROKEclean[\"overall_diagnosis\"] = STROKEclean[\"EmergencyDepartmentDiagnosis\"].astype(str) + STROKEclean[\"HospitalDiagnosis\"].astype(str)\n",
    "STROKEclean[\"Merged_Impression\"] = STROKEclean[\"PrimaryImpression\"].fillna('') + STROKEclean[\"SecondaryImpression\"].fillna('')\n",
    "\n",
    "##drop reduntant columns\n",
    "#STROKE.drop([\"EmergencyDepartmentDiagnosis\", \"HospitalDiagnosis\"], axis =1,  inplace =True)\n",
    "\n",
    "##View Header and count null\n",
    "\n",
    "print(\"\\n header:\\n\",STROKEclean.head(2))\n",
    "\n",
    "print(\"\\n count null after merge:\\n\", STROKEclean.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c47f7b6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#STROKEclean.dtypes\n",
    "print(\"\\n count NaN after merge:\\n\",STROKEclean[STROKEclean == 'nannan'].count())\n",
    "#STROKEclean['overall_diagnosis'].eq('nannan').sum()##df1[['overall_diagnosis ', 'col3', 'col4']].eq(1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decc574e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#get the unique values\n",
    "#PI = pd.DataFrame(STROKEclean[\"PrimaryImpression\"].unique())\n",
    "#PI\n",
    "# STROKEclean['Uniques'] = pd.DataFrame(STROKEclean[\"SecondaryImpression\"].unique())\n",
    "# #SI[0] = SI['Uniques']\n",
    "# #SI.drop([0])\n",
    "# #SI.describe()\n",
    "# STROKEclean.dtypes\n",
    "# STROKEclean.head()\n",
    "#df.drop(columns=['B', 'C'])\n",
    "#STROKEclean.drop(['Uniques'], axis=1)\n",
    "#Write as .csv\n",
    "#SI.to_excel(\"E:\\DAEN_690\\Data\\SI.xlsx\",sheet_name='STROKE' , index=False)\n",
    "\n",
    "STROKEclean[\"Merged_Impression\"].unique() \n",
    "# I63"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac52c3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#THe distinct impressions related to strokes are:\n",
    "## Primary Impression / ##Secondary Impression\n",
    "\n",
    "#['Neuro - Stroke/CVA (I63.9)',\n",
    "#'Intracranial - Stroke (CVA) Hemorrhagic (I62.9)',\n",
    "#in combination with\n",
    "#,'Environment - Heat Exhaustion/Stroke (T67.5)',\n",
    "# 'Environment - Heatstroke (T67.0)']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7530dcde",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## create a new column that is 1 if the overall_diagnosis column contains the string \"A41.9\", 0 if not\n",
    "#https://www.icd10data.com/ICD10CM/Codes/I00-I99/I60-I69/I63-/I63.9#:~:text=Cerebral%20infarction%2C%20unspecified,-2016%202017%202018&text=Billable%2FSpecific%20Code-,I63.,effective%20on%20October%201%2C%202021.\n",
    "#ICD-10 Categories I60-I69 Cerebrovascular Disease: #https://providers.bcbsal.org/portal/documents/10226/306297/Correctly+Coding+Cerebrovascular+Disease/87166c14-7be2-4728-b32b-95ec8f802fea\n",
    "\n",
    "#in the data set I62.9 & I63.9 are the only distinct codes used for Stroke.\n",
    "STROKEclean[\"STROKE_outcome\"] = pd.np.where(STROKEclean[\"overall_diagnosis\"].str.contains(\"I6\", na=False, case=False), \"1\", \"0\")#)#)\n",
    "\n",
    "#pd.np.where(STROKEclean[\"overall_diagnosis\"].str.contains(\"STROKE\", na=False, case=False), \"1\", \n",
    "                                #pd.np.where(STROKEclean[\"overall_diagnosis\"].str.contains(\"STROKE\", na=False), \"1\",\n",
    "\n",
    "    \n",
    "    \n",
    "print(STROKEclean.head())\n",
    "#Write as .csv\n",
    "STROKEclean.to_excel(\"E:\\DAEN_690\\Data\\PdmSTROKE.xlsx\",sheet_name='STROKE' , index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a8286b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## create a new column that is Sepsis if the PrimaryImpression column contains the string \"Sepsis\", Other if not\n",
    "\n",
    "STROKEclean[\"Overall_Impression\"] = pd.np.where(STROKEclean[\"Merged_Impression\"].str.contains(\"I6\", na=False, case=False), 1, 0)#)\n",
    "                               \n",
    "                             \n",
    "        #pd.np.where(STROKEclean[\"PrimaryImpression\"].str.contains(\"STROKE\", na=False, case=False), \"STROKE\", \n",
    "print(STROKEclean.head())\n",
    "#Write as .csv\n",
    "STROKEclean.to_excel(\"E:\\DAEN_690\\Data\\STROKEclean.xlsx\",sheet_name='STROKE' , index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effcad0b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"\\n PImpressions Values: \\n\", STROKEclean['Merged_Impression'].unique())\n",
    "print(\"\\n PImpr Values: \\n\", STROKEclean['Overall_Impression'].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172a5825",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Drop redundant column \n",
    "STROKEclean.drop(['MonthYear','PrimaryImpression','SecondaryImpression', \"EmergencyDepartmentDiagnosis\",\"HospitalDiagnosis\",\"overall_diagnosis\"], axis =1,  inplace =True)\n",
    "STROKEclean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017101bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "STROKEclean.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947938f8",
   "metadata": {},
   "source": [
    "Merge with Patient Demographic Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5940ff18",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# merging the files how='inner' by default.\n",
    "PdmSTROKE = pd.merge(PatDemo,STROKE, on='PtIDInternal') \n",
    "print(PdmSTROKE.head())\n",
    "#Write as .csv\n",
    "PdmSTROKE.to_excel(\"E:\\DAEN_690\\Data\\PdmSTROKE.xlsx\",sheet_name='STROKE' , index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f026ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import altair as alt\n",
    "#from vega_datasets import data\n",
    "\n",
    "source = PdmSTROKE\n",
    "\n",
    "PatSTROKEChart = alt.Chart(source).mark_bar().encode(\n",
    "    x='AgeGroup_HIPAA:O',\n",
    "    y='count(AgeGroup_HIPAA):Q',\n",
    "    color='PrimaryImpr:N',\n",
    "    column='StrokeStatus:N',\n",
    "    tooltip=['PrimaryImpr', 'STROKE_outcome', 'AgeGroup_HIPAA', 'count(AgeGroup_HIPAA)']\n",
    ").interactive()\n",
    "PatSTROKEChart .display()\n",
    "#PatDemChart.save('PatDemChart.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86ffe3b",
   "metadata": {},
   "source": [
    "#Final Cleaning of the DataSets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e4786f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Filter the dataframes to only keep rows where \"isOutcomeFacility\" = 1\n",
    "## this is because it doesn't make sense to count these rows which all have NULL for diagnosis when doing analysis\n",
    "# df.loc[df['column_name'] == some_value]\n",
    "PdmSEPSIS = PdmSEPSIS.loc[PdmSEPSIS['isOutcomeFacility'] == 1]\n",
    "PdmSTEMI = PdmSTEMI.loc[PdmSTEMI['isOutcomeFacility'] == 1]\n",
    "PdmSTROKE = PdmSTROKE.loc[PdmSTROKE['isOutcomeFacility'] == 1]\n",
    "\n",
    "## Filter the dataframes to get rid of rows where overall_diagnosis is not null\n",
    "PdmSEPSIS = PdmSEPSIS[PdmSEPSIS['overall_diagnosis'].str.len() > 0]\n",
    "PdmSTEMI = PdmSTEMI[PdmSTEMI['overall_diagnosis'].str.len() > 0]\n",
    "PdmSTROKE = PdmSTROKE[PdmSTROKE['overall_diagnosis'].str.len() > 0]\n",
    "\n",
    "# df = df[df['str_field'].str.len() > 0]\n",
    "\n",
    "sepsis_demo.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ed00bf",
   "metadata": {},
   "source": [
    "# Finding out the common patients across all data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0c4647",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# merging the files how='inner' by default.\n",
    "#PdmMerged = pd.merge(PatDemo,SEPSIS,STEMI,STROKE, on='PtIDInternal') \n",
    "PdmMerged = PatDemo.merge(SEPSIS, on='PtIDInternal').merge(STEMI,on='PtIDInternal').merge(STROKE,on='PtIDInternal')\n",
    "PdmMerged \n",
    "\n",
    "#Alternatively\n",
    "#df4 = pd.merge(pd.merge(PatDemo,SEPSIS,on='PtIDInternal'),pd.merge(STEMI,STROKE,on='PtIDInternal'))\n",
    "# print(df4)\n",
    "# #Write as .csv\n",
    "# df4.to_excel(\"E:\\DAEN_690\\Data\\RepeatingPat.xlsx\",sheet_name='RepeatingPat' , index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7c0e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import altair as alt\n",
    "# from vega_datasets import data\n",
    "\n",
    "# chart = alt.Chart(data.cars.url).mark_point().encode(\n",
    "#     x='Horsepower:Q',\n",
    "#     y='Miles_per_Gallon:Q',\n",
    "#     color='Origin:N'\n",
    "# )\n",
    "\n",
    "# chart.save('chart.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d267e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    " #Anaconda Prompt-jupyter notebook list-for token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc03e481",
   "metadata": {},
   "source": [
    "#Analytics & Algorithms/ EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f32387",
   "metadata": {},
   "source": [
    "SEPSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f749be5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#https://medium.com/@szabo.bibor/how-to-create-a-seaborn-correlation-heatmap-in-python-834c0686b88e\n",
    "#Checking the correlations among attributes.\n",
    "\n",
    "plt.figure(figsize=(20,20))\n",
    "sns.heatmap(PdmSEPSIS.corr(),cbar=True,annot=True,cmap='YlGnBu') #vmin=0.3, vmax=0.99,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed339725",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/61956336/find-high-correlations-in-a-large-coefficient-matrix\n",
    "#https://stackoverflow.com/questions/67711552/python-split-pandas-dataframe-by-range-of-values\n",
    "\n",
    "coeff = PdmSEPSIS.corr()\n",
    "\n",
    "# 0.3 is used for illustration \n",
    "# replace with your actual value\n",
    "thresh_low = 0.3\n",
    "thresh_high = 1\n",
    "\n",
    "#mask = coeff.abs().lt(thresh)\n",
    "# or mask = coeff> thresh\n",
    "mask = (coeff> thresh_low) & (coeff < thresh_high) \n",
    "\n",
    "coeff.where(mask).stack()\n",
    "\n",
    "# #Remove Diagonol\n",
    "# corr = PdmSEPSIS.corr()\n",
    "# import numpy as np\n",
    "# np.fill_diagonal(corr.values, np.nan)\n",
    "# print(corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924ec47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot only the Correlations between 0.3 and 1.0\n",
    "\n",
    "CorCoeff = coeff.where(mask)\n",
    "\n",
    "plt.figure(figsize=(20,20))\n",
    "sns.heatmap(CorCoeff,cbar=True,annot=True,cmap='YlGnBu') #.reshape(26,1)\n",
    "plt.title('Correlations between 0.3 and 1.0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14837ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Heat map of Nans\n",
    "plt.figure(figsize=(20,10))\n",
    "sns.heatmap(CorCoeff.isnull(), cbar=False, cmap=\"YlGnBu\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205016d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets drop irrelavant  columns:\n",
    "PdmSEPSIS.drop(['Accuracy','Triage'],axis =1,  inplace =True)\n",
    "print(PdmSEPSIS.head(3))\n",
    "print(\"\\n info: \", PdmSEPSIS.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b47232",
   "metadata": {},
   "source": [
    "Prepare Data Set for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41258165",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now you have to take off the Response column into y\n",
    "import pandas as pd     # manipulate dataframe\n",
    "import seaborn as sns   # visualization of classification\n",
    "\n",
    "from sklearn.model_selection import train_test_split   # to split the data\n",
    "from sklearn.linear_model import LogisticRegression    # to bring logistic regression\n",
    "from sklearn.tree import DecisionTreeClassifier        # to bring decision tree\n",
    "from sklearn.ensemble import RandomForestClassifier    # to bring random forest\n",
    "from sklearn.model_selection import GridSearchCV       # to find best hyper parameters\n",
    "\n",
    "from sklearn import metrics                            # to create confusion matrix\n",
    "\n",
    "X = PdmSEPSIS.drop('sepsis_outcome',axis=1)\n",
    "y = PdmSEPSIS['sepsis_outcome']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4d2589",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"\\n Xhead: \",X.head(3))\n",
    "print (\"\\n yhead: \",y.head(3))       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5b9f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "#And then we are going to split this X and y into train and test data.\n",
    "#https://towardsdatascience.com/why-do-we-set-a-random-state-in-machine-learning-models-bb2dc68d8431\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 1357)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20fbaa60",
   "metadata": {},
   "source": [
    "# Logistic Regression & Multinomial Logistic Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154626eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.analyticsvidhya.com/blog/2017/08/skilltest-logistic-regression/\n",
    "#Yes, we can apply logistic regression on 3 classification problem, We can use One Vs all method for 3 class classification in logistic regression.\n",
    "#https://machinelearningmastery.com/multinomial-logistic-regression-with-python/\n",
    "\n",
    "LR = LogisticRegression()    # Bring empty logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26790089",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic regression uses maximum likely hood estimate for training a logistic regression.\n",
    "\n",
    "LR.fit(x_train,y_train)      # Train the model with training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5396eeb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_LR = LR.predict(x_test)    # Get predicted y from the logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ced5465",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the classification accuracy, Recall, and Presicion with the metrics function.\n",
    "LR_accuracy = metrics.accuracy_score(y_pred_LR,y_test)\n",
    "print('Classification accuracy = ',LR_accuracy)\n",
    "print('Recall = ',metrics.recall_score(y_test, y_pred_LR, average='weighted')) #average='macro' #average='micro' #average='weighted'\n",
    "print('Precision = ',metrics.precision_score(y_test, y_pred_LR, average='weighted')) #average='macro' #average='micro' #average='weighted'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfdf899f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Check the class lable distribution\n",
    "ytrain = pd.DataFrame(y_train)\n",
    "\n",
    "Class_Distribution = pd.DataFrame(ytrain['sepsis_outcome'].value_counts())\n",
    "Class_Distribution['Percentages'] = (Class_Distribution['sepsis_outcome']/Class_Distribution['sepsis_outcome'].sum())*100\n",
    "Class_Distribution\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca3592b",
   "metadata": {},
   "source": [
    "# Multinomial Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09fc7ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate multinomial logistic regression model\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# define dataset\n",
    "#X, y = make_classification(n_samples=1000, n_features=10, n_informative=5, n_redundant=5, n_classes=3, random_state=1)\n",
    "X, y = x_train, y_train\n",
    "# define the multinomial logistic regression model\n",
    "model = LogisticRegression(multi_class='multinomial', solver='lbfgs')\n",
    "# define the model evaluation procedure\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# evaluate the model and collect the scores\n",
    "n_scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "# report the model performance\n",
    "print('Mean Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89142252",
   "metadata": {},
   "source": [
    "logistic regression model with default penalty achieved a mean classification accuracy of about 91 percent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739b9273",
   "metadata": {},
   "source": [
    "calling the predict() function to make a prediction for new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88decda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import RandomForestRegressor\n",
    "# clf = RandomForestRegressor()\n",
    "# clf.fit(X,y)\n",
    "\n",
    "##https://scikit-learn.org/stable/auto_examples/linear_model/plot_ols.html\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# # Load the diabetes dataset\n",
    "# diabetes_X, diabetes_y = datasets.load_diabetes(return_X_y=True)\n",
    "\n",
    "# # Use only one feature\n",
    "# diabetes_X = diabetes_X[:, np.newaxis, 2]\n",
    "\n",
    "# # Split the data into training/testing sets\n",
    "# diabetes_X_train = diabetes_X[:-20]\n",
    "# diabetes_X_test = diabetes_X[-20:]\n",
    "\n",
    "# # Split the targets into training/testing sets\n",
    "# diabetes_y_train = diabetes_y[:-20]\n",
    "# diabetes_y_test = diabetes_y[-20:]\n",
    "\n",
    "# # Create linear regression object\n",
    "# regr = linear_model.LinearRegression()\n",
    "\n",
    "# # Train the model using the training sets\n",
    "# regr.fit(diabetes_X_train, diabetes_y_train)\n",
    "\n",
    "# # Make predictions using the testing set\n",
    "# diabetes_y_pred = regr.predict(diabetes_X_test)\n",
    "\n",
    "# # The coefficients\n",
    "# print(\"Coefficients: \\n\", regr.coef_)\n",
    "# # The mean squared error\n",
    "# print(\"Mean squared error: %.2f\" % mean_squared_error(diabetes_y_test, diabetes_y_pred))\n",
    "# # The coefficient of determination: 1 is perfect prediction\n",
    "# print(\"Coefficient of determination: %.2f\" % r2_score(diabetes_y_test, diabetes_y_pred))\n",
    "\n",
    "# # Plot outputs\n",
    "# plt.scatter(diabetes_X_test, diabetes_y_test, color=\"black\")\n",
    "# plt.plot(diabetes_X_test, diabetes_y_pred, color=\"blue\", linewidth=3)\n",
    "\n",
    "# plt.xticks(())\n",
    "# plt.yticks(())\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6863d91a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#type(x_test)\n",
    "\n",
    "# make a prediction with a multinomial logistic regression model\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# define dataset\n",
    "#X, y = make_classification(n_samples=1000, n_features=10, n_informative=5, n_redundant=5, n_classes=3, random_state=1)\n",
    "X, y = x_train, y_train\n",
    "# define the multinomial logistic regression model\n",
    "model = LogisticRegression(multi_class='multinomial', solver='lbfgs')\n",
    "# fit the model on the whole dataset\n",
    "model.fit(X, y)\n",
    "# define a single row of input data\n",
    "##row = [1.89149379, -0.39847585, 1.63856893, 0.01647165, 1.51892395, -3.52651223, 1.80998823, 0.58810926, -0.02542177, -0.52835426]\n",
    "row = x_test\n",
    "## predict the class label\n",
    "#yhat = model.predict([row])\n",
    "yhatt = model.predict(row)\n",
    "## summarize the predicted class\n",
    "#print('Predicted Class: %d' % yhat[0])\n",
    "print(\"\\n yhat: \", yhatt[0])\n",
    "\n",
    "# The coefficients\n",
    "print(\"\\n Coefficients: \\n\", model.coef_)\n",
    "\n",
    "# The mean squared error\n",
    "#print(\"Mean squared error: %.2f\" % mean_squared_error(y_test, yhat)) #could not convert string to float: 'UnderTriage'\n",
    "# The coefficient of determination: 1 is perfect prediction\n",
    "#print(\"Coefficient of determination: %.2f\" % r2_score(y_test, yhat)) #ValueError: could not convert string to float: 'UnderTriage'\n",
    "\n",
    "### Plot outputs\n",
    "# plt.scatter(x_test, y_test, color=\"black\")\n",
    "# plt.plot(x_test, yhat, color=\"blue\", linewidth=3)\n",
    "# plt.xticks(())\n",
    "# plt.yticks(())\n",
    "# plt.show() #ValueError: x and y must be the same size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347c0e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#type(yhat)\n",
    "#yhat.shape #(633,)\n",
    "#yhat.dtype\n",
    "#yhat[1]\n",
    "np.where(yhatt == 0) #Check for the indexes\n",
    "#np.where(yhatt == 'UnderTriage')\n",
    "#np.where(yhatt == 'OverTriage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26bf21de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### predict a multinomial probability distribution\n",
    "yhat = model.predict_proba(row)\n",
    "# summarize the predicted probabilities\n",
    "print('\\n Predicted Probabilities: %s' % yhat[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e153eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Class:        Predicted Probabilities:\n",
    "        \n",
    "#1 :         Predicted Probabilities: [0.00303091 0.99696909]\n",
    "#0:          Predicted Probabilities: [0.00342926 0.99657074]\n",
    "\n",
    "# Triage:        Predicted Probabilities: [0.00330011 0.99221901 0.00448088]\n",
    "\n",
    "# OverTriage:    Predicted Probabilities: [0.9918642  0.00597873 0.00215706]\n",
    "\n",
    "# UnderTriage:   Predicted Probabilities: [9.15067953e-04 2.14479714e-03 9.96940135e-01]  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3fa119",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#https://machinelearningmastery.com/multinomial-logistic-regression-with-python/\n",
    "# tune regularization for multinomial logistic regression\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from matplotlib import pyplot\n",
    "\n",
    "\n",
    "# get the dataset\n",
    "def get_dataset():\n",
    "    #X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=1, n_classes=3)\n",
    "    X, y = x_train, y_train\n",
    "    return X, y\n",
    "\n",
    "# get a list of models to evaluate\n",
    "def get_models():\n",
    "    models = dict()\n",
    "    for p in [0.0, 0.0001, 0.001, 0.01, 0.1, 1.0]:\n",
    "        # create name for model\n",
    "        key = '%.4f' % p\n",
    "        # turn off penalty in some cases\n",
    "        if p == 0.0:\n",
    "            # no penalty in this case\n",
    "            models[key] = LogisticRegression(multi_class='multinomial', solver='lbfgs', penalty='none')\n",
    "        else:\n",
    "            models[key] = LogisticRegression(multi_class='multinomial', solver='lbfgs', penalty='l2', C=p)\n",
    "    return models\n",
    " \n",
    "# evaluate a give model using cross-validation\n",
    "def evaluate_model(model, X, y):\n",
    "    # define the evaluation procedure\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "    # evaluate the model\n",
    "    scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "    return scores\n",
    " \n",
    "# define dataset\n",
    "X, y = get_dataset()\n",
    "# get the models to evaluate\n",
    "models = get_models()\n",
    "# evaluate the models and store results\n",
    "results, names = list(), list()\n",
    "for name, model in models.items():\n",
    "    # evaluate the model and collect the scores\n",
    "    scores = evaluate_model(model, X, y)\n",
    "    # store the results\n",
    "    results.append(scores)\n",
    "    names.append(name)\n",
    "    # summarize progress along the way\n",
    "    print('>%s %.3f (%.3f)' % (name, mean(scores), std(scores)))\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04db8668",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot model performance for comparison\n",
    "pyplot.boxplot(results, labels=names, showmeans=True)\n",
    "pyplot.title('L2 Penalty Configuration vs. Accuracy for Multinomial Logistic Regression')\n",
    "pyplot.show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea0a46a",
   "metadata": {},
   "source": [
    "\n",
    "#larger penalty we use on this dataset- the smaller the C value - the worse the performance of the model.\n",
    "#L2 penalty with weighting values in the range from 0.0001 to 1.0 on a log scale\n",
    "#no penalty or 0.0\n",
    "C value 1.0 has the better accuracy score of 100% than no penalty of 0.00\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeafd35a",
   "metadata": {},
   "source": [
    "# Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1efd8685",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://towardsdatascience.com/support-vector-machines-explained-with-python-examples-cb65e8172c85\n",
    "#SVM \n",
    "\n",
    "#Import Library\n",
    "from sklearn import svm\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "X=x_train\n",
    "\n",
    "y=y_train\n",
    "\n",
    "model = svm.SVC() \n",
    "model.fit(X, y)\n",
    "model.score(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0262f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#predict \n",
    "print(\"\\n predicted:\\n\\n\", model.predict(x_test))\n",
    "# get support vectors\n",
    "print(\"\\n support_vectors:\\n\\n\",model.support_vectors_)\n",
    "# get indices of support vectors\n",
    "print(\"\\n support:\\n\\n\",model.support_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd341167",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Multi-class classification\n",
    "#https://www.baeldung.com/cs/svm-multiclass-classification#:~:text=SVM%20Multiclass%20Classification%20in%20Python&text=We%20developed%20two%20different%20classifiers,on%20the%20same%20data%20set.\n",
    "from sklearn import svm, datasets\n",
    "import sklearn.model_selection as model_selection\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "\n",
    "\n",
    "#create two different classifiers, Polynomial kernel, and another one with RBF kernel\n",
    "rbf = svm.SVC(kernel='rbf', gamma=0.5, C=0.1).fit(x_train, y_train)\n",
    "poly = svm.SVC(kernel='poly', degree=3, C=1).fit(x_train, y_train)\n",
    "\n",
    "#calculate the efficiency of the two models\n",
    "\n",
    "poly_pred = poly.predict(x_test)\n",
    "rbf_pred = rbf.predict(x_test)\n",
    "\n",
    "#calculate the accuracy and f1 scores for SVM with Polynomial kernel\n",
    "\n",
    "poly_accuracy = accuracy_score(y_test, poly_pred) #percentage of the true positive and true negative to all data points\n",
    "poly_f1 = f1_score(y_test, poly_pred, average='weighted') #harmonic mean between precision and recall, and both depend on the false positive and false negative\n",
    "print('Accuracy (Polynomial Kernel): ', \"%.2f\" % (poly_accuracy*100))\n",
    "print('F1 (Polynomial Kernel): ', \"%.2f\" % (poly_f1*100))\n",
    "\n",
    "#accuracy and f1 scores for SVM with RBF kernel:\n",
    "rbf_accuracy = accuracy_score(y_test, rbf_pred) #percentage of the true positive and true negative to all data points\n",
    "rbf_f1 = f1_score(y_test, rbf_pred, average='weighted') #harmonic mean between precision and recall, and both depend on the false positive and false negative\n",
    "print('Accuracy (RBF Kernel): ', \"%.2f\" % (rbf_accuracy*100))\n",
    "print('F1 (RBF Kernel): ', \"%.2f\" % (rbf_f1*100))\n",
    "\n",
    "#SVM hyperparameters, like C, gamma, and degree \n",
    "#f1 score when the data set isnt balanced\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b8b295",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get number of support vectors for each class\n",
    "model.n_support_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51ff720",
   "metadata": {},
   "source": [
    "# DECISION TREE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd44000",
   "metadata": {},
   "outputs": [],
   "source": [
    "DTC = DecisionTreeClassifier()    # Bring empty decision tree model\n",
    "                                  # you can set the name whatever you want. it doesn't need to be \"DTC\"\n",
    "    \n",
    "DTC.fit(x_train,y_train)          # Train the decision tree model with training data\n",
    "\n",
    "y_pred_DTC = DTC.predict(x_test)  # Get predicted y from the decision tree model\n",
    "\n",
    "Decision_accuracy = metrics.accuracy_score(y_pred_DTC,y_test)  # Get the classification accuracy\n",
    "print(Decision_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d031c3",
   "metadata": {},
   "source": [
    "GRID SEARCH METHOD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5135b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_DTC = DecisionTreeClassifier()    # Bring empty decision tree\n",
    "\n",
    "# Prepare the combination of parameters.\n",
    "param_dist={\"criterion\":[\"gini\",\"entropy\"],       # 2 options for criterion\n",
    "            'max_depth': range(2,8),              # 6 options for max_depth\n",
    "            'max_leaf_nodes': range(50,150,50)}   # 2 options for max_leaf_nodes\n",
    "# So, the total number of combination is 2*6*2 = 24\n",
    "\n",
    "# cv means cross validation and this concept will come in the future\n",
    "grid_DTC = GridSearchCV(grid_DTC, param_grid= param_dist, cv=5, n_jobs=-1)   \n",
    "#Number of jobs to run in parallel.  -1 means using all processors. \n",
    "\n",
    "grid_DTC.fit(x_train, y_train)     # Fit all of the 24 combinations\n",
    "\n",
    "\n",
    "best_parameters = grid_DTC.best_params_   # Get best parameters \n",
    "print(best_parameters)\n",
    "\n",
    "# Create a decision tree with best parameters and train it with whole train data\n",
    "after_grid_DTC = DecisionTreeClassifier(criterion= 'entropy', max_depth= 6, max_leaf_nodes= 50, random_state= 1357)\n",
    "after_grid_DTC = after_grid_DTC.fit(x_train, y_train)\n",
    "\n",
    "y_pred_grid = after_grid_DTC.predict(x_test)     # Get predicted y from the decision tree model\n",
    "\n",
    "Decision_accuracy = metrics.accuracy_score(y_pred_grid,y_test)\n",
    "print(Decision_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e137c9d1",
   "metadata": {},
   "source": [
    "# RANDOM FOREST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ac59d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "RF = RandomForestClassifier(100,n_jobs=-1,random_state=1357)   # Bring empty random forest model, 100 is the number of trees in the RF\n",
    "RF.fit(x_train,y_train)   # Train the model with training data\n",
    "\n",
    "y_pred_RF = RF.predict(x_test)   # Get predicted y from the random forest model\n",
    "\n",
    "# Get the classification accuracy, Recall, and Presicion with the metrics function.\n",
    "print('Accucary = ',metrics.accuracy_score(y_pred_RF,y_test))\n",
    "print('Recall = ',metrics.recall_score(y_test, y_pred_RF, average='weighted'))#average='macro' #average='micro' #average='weighted'\n",
    "print('Precision = ',metrics.precision_score(y_test, y_pred_RF, average='weighted'))#average='macro' #average='micro' #average='weighted'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0440c77d",
   "metadata": {},
   "source": [
    "Visualize the confusion metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43e768e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = metrics.confusion_matrix(y_test, y_pred_RF)    # Create a confusion matrix from your predicted y and original y value.\n",
    "                                                    # In this code, we use the result of random forest, but you can replace with another classifier\n",
    "\n",
    "sns.heatmap(cm,annot=True,cmap='Reds',fmt='.0f')    # Bring heatmap from the seaborn and fit our confusion matrix\n",
    "                                                    # annot = True can shows the confusion matrix values\n",
    "                                                    # camp can decide the color of heatmap\n",
    "                                                    # fmt can decide the type of presentation of confusion matrix values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8aa6c0",
   "metadata": {},
   "source": [
    "ROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ebae58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With false_positive_rate and true_positive_rate, we can plot the ROC curve.\n",
    "\n",
    "# To get false_positive_rate and true_positive_rate, we need to use predicted probability and actual labels.\n",
    "\n",
    "# y_pred_proba = RF.predict_proba(x_test)   # Get the probability of classification from the Random Forest model\n",
    "#                                           # In this code, we use the result of random forest, but you can replace with another classifier\n",
    "\n",
    "# false_positive_rate, true_positive_rate, thresholds = metrics.roc_curve(y_test, y_pred_proba[:,1])     \n",
    "# # Orignally predict_proba return 2 values for both class. \n",
    "# # So, we picked second one which give the probability of class 1 #ValueError: multiclass format is not supported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dfa38ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# %matplotlib inline  \n",
    "# plt.ylabel('True Positive Rate')\n",
    "# plt.xlabel('False Positive Rate')\n",
    "# plt.title('ROC')\n",
    "# plt.plot(false_positive_rate, true_positive_rate)  \n",
    "# # By simply ploting false positive rate and true positive rate, you can plot the roc curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d2e72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Area Under the Curve\n",
    "# auc = metrics.auc(false_positive_rate, true_positive_rate)   # metrics also privide the auc area\n",
    "# print(auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931ae0a7",
   "metadata": {},
   "source": [
    "# Vizualizations:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3a5bdf",
   "metadata": {},
   "source": [
    "Demography distributions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2edbe2e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Age group distribution among different demographies\n",
    "\n",
    "import altair as alt\n",
    "\n",
    "\n",
    "source = PatDemo\n",
    "\n",
    "PtDemchart1 = alt.Chart(source).mark_bar().encode(\n",
    "    x='AgeGroup_HIPAA:O',\n",
    "    y='count(AgeGroup_HIPAA):Q',\n",
    "    color='PatientEthnicity:N'\n",
    ")\n",
    "\n",
    "\n",
    "# PtDemchart2 = alt.Chart(PatDemo).mark_bar().encode(\n",
    "#      y='AgeGroup_HIPAA',\n",
    "#      x='count(AgeGroup_HIPAA)',\n",
    "#     color='PatientRace:N'\n",
    "#  )\n",
    "\n",
    "PtDemchart3 = alt.Chart(PatDemo).mark_bar().encode(\n",
    "     x='AgeGroup_HIPAA:O',\n",
    "     y='count(AgeGroup_HIPAA):Q',\n",
    "    color='PatientSex:N'\n",
    " )\n",
    "\n",
    "\n",
    "PtDemchart1 | PtDemchart3#PtDemchart2|\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866646d6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#interactive brush selection charts\n",
    "import altair as alt\n",
    "from vega_datasets import data\n",
    "alt.data_transformers.disable_max_rows()\n",
    "alt.data_transformers.enable('json')\n",
    "#pip install altair_data_server\n",
    "#alt.data_transformers.enable('data_server')\n",
    "\n",
    "source = PatDemo\n",
    "brush = alt.selection(type='interval')\n",
    "\n",
    "alt.Chart(source).mark_point().encode(\n",
    "    x='AgeGroup_HIPAA:O',\n",
    "    y='count(AgeGroup_HIPAA):Q',\n",
    "    color=alt.condition(brush, 'PatientRace:O', alt.value('grey')),\n",
    ").add_selection(brush)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f44846",
   "metadata": {},
   "outputs": [],
   "source": [
    "#interactive Charts\n",
    "\n",
    "import altair as alt\n",
    "from vega_datasets import data\n",
    "\n",
    "source = PatDemo\n",
    "selection = alt.selection_multi(fields=['PatientSex'], bind='legend')\n",
    "PatDemChart = alt.Chart(source).mark_bar().encode(\n",
    "    x='AgeGroup_HIPAA:O',\n",
    "    y='count(AgeGroup_HIPAA):Q',\n",
    "    color='PatientSex:N',\n",
    "    column='PatientRace:N',\n",
    "    tooltip=['PatientEthnicity', 'PatientSex', 'AgeGroup_HIPAA', 'count(AgeGroup_HIPAA)']\n",
    ").add_selection(\n",
    "    selection\n",
    ")\n",
    "PatDemChart .display()\n",
    "#PatDemChart.save('PatDemChart.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afee9421",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Interactive Agegroup distribution among Patient Race and Gender #https://altair-viz.github.io/gallery/streamgraph.html\n",
    "import altair as alt\n",
    "from vega_datasets import data\n",
    "\n",
    "#source = pd.read_csv(\"E:\\DAEN_690\\Data\\us-employment.csv\")\n",
    "\n",
    "source = PatDemo\n",
    "selection = alt.selection_multi(fields=['PatientRace'], bind='legend')\n",
    "\n",
    "PatDemInteractive= alt.Chart(source).mark_area().encode(\n",
    "    alt.X('AgeGroup_HIPAA:O', axis=alt.Axis(domain=False, tickSize=0)),\n",
    "    alt.Y('count(AgeGroup_HIPAA):Q', stack='center', axis=None),\n",
    "    alt.Color('PatientRace:N', scale=alt.Scale(scheme='category20b')),\n",
    "    opacity=alt.condition(selection, alt.value(1), alt.value(0.2)),\n",
    "    tooltip=['PatientSex:N','PatientRace:N', 'AgeGroup_HIPAA:O', 'count(AgeGroup_HIPAA):Q']\n",
    "  \n",
    ").add_selection(\n",
    "    selection\n",
    ").add_selection(brush).interactive()\n",
    "\n",
    "#PatDemInteractive.display()\n",
    "#PatDemInteractive.save('chart.html', embed_options={'renderer':'svg'})\n",
    "PatDemInteractive.display()\n",
    "#chart.save('chart.png', scale_factor=2.0) #Size Resolutions\n",
    "\n",
    "#PatDemInteractive.save('chart.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2774efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Second viz\n",
    "source = PatDemo\n",
    "selection = alt.selection_multi(fields=['PatientRace'], bind='legend')\n",
    "\n",
    "PatDemInteractive1= alt.Chart(source).mark_area().encode(\n",
    "    alt.X('AgeGroup_HIPAA:O', axis=alt.Axis(domain=False, tickSize=0)),\n",
    "    alt.Y('count(AgeGroup_HIPAA):Q', stack='center', axis=None),\n",
    "    alt.Color('PatientEthnicity:N', scale=alt.Scale(scheme='category20b')),\n",
    "    opacity=alt.condition(selection, alt.value(1), alt.value(0.2)),\n",
    "    tooltip=['PatientSex','PatientEthnicity', 'AgeGroup_HIPAA', 'count(AgeGroup_HIPAA)']\n",
    "  \n",
    ").add_selection(\n",
    "    selection\n",
    ").add_selection(brush).interactive()\n",
    "\n",
    "PatDemInteractive1.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05164441",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Age group among gender distribution\n",
    "\n",
    "import altair as alt\n",
    "from vega_datasets import data #Only to use data from vegas data sets or else can be ignored\n",
    "\n",
    "source = PatDemo\n",
    "\n",
    "Agegroup = alt.Chart(source).mark_circle(size=60).encode(\n",
    "    x='AgeGroup_HIPAA:O',\n",
    "    y='count(AgeGroup_HIPAA):Q',\n",
    "    color='PatientSex:N',\n",
    "    tooltip=['PatientRace:N', 'PatientSex:N', 'AgeGroup_HIPAA:O', 'count(AgeGroup_HIPAA):Q']\n",
    ").interactive()\n",
    "Agegroup.save('Agegroup.html')\n",
    "Agegroup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf68680",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://towardsdatascience.com/dash-for-beginners-create-interactive-python-dashboards-338bfcb6ffa4\n",
    "#Plotly Dash\n",
    "#pip install dash --Anaconda Prompt\n",
    "#pip install jupyter-dash\n",
    "# ! pip install dash-html-components                                         \n",
    "# ! pip install dash-core-components                                     \n",
    "# ! pip install plotly\n",
    "# from dash import Dash, html, dcc, Input, Output\n",
    "# import pandas as pd\n",
    "# import plotly.express as px\n",
    "#https://plotly.com/python/dot-plots/\n",
    "#import dash package\n",
    "\n",
    "import dash\n",
    "import dash_html_components as html\n",
    "import plotly.graph_objects as go\n",
    "import dash_core_components as dcc\n",
    "import plotly.express as px\n",
    "from dash.dependencies import Input, Output\n",
    "import plotly.express as px\n",
    "\n",
    "df = PdmSEPSIS \n",
    "\n",
    "fig = px.scatter(df, y=\"ProviderImpression\", x=\"sepsis_outcome\", color=\"Triage\", symbol = \"Triage\")\n",
    "fig.update_traces(marker_size=10)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7ec0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import altair as alt\n",
    "from vega_datasets import data\n",
    "\n",
    "source = PdmSEPSIS\n",
    "\n",
    "alt.Chart(source).mark_circle(size=60).encode(\n",
    "    x='ProviderImpression',\n",
    "    y='Triage',\n",
    "    color='SepsisStatus_Alert',\n",
    "    tooltip=['PatientSex_Female', 'SepsisStatus_Alert', 'ProviderImpression', 'Triage']\n",
    ").interactive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8cabf9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import altair as alt\n",
    "from vega_datasets import data\n",
    "\n",
    "source = PdmSEPSIS\n",
    "\n",
    "alt.Chart(source).mark_circle().encode(\n",
    "    alt.X(alt.repeat(\"column\"), type='quantitative'),\n",
    "    alt.Y(alt.repeat(\"row\"), type='quantitative'),\n",
    "    color='Triage:N'\n",
    ").properties(\n",
    "    width=150,\n",
    "    height=150\n",
    ").repeat(\n",
    "    row=['SepsisStatus_Alert', 'ProviderImpression', 'Accuracy'],\n",
    "    column=['Accuracy', 'ProviderImpression', 'SepsisStatus_Alert']\n",
    ").interactive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b664f73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import altair as alt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "source = PdmSEPSIS\n",
    "\n",
    "base = alt.Chart(source).mark_circle(opacity=0.5).transform_fold(\n",
    "    fold=['Accuracy', 'ProviderImpression', 'SepsisStatus_Alert'],\n",
    "    as_=['category', 'y']\n",
    ").encode(\n",
    "    alt.X('Triage:N'),\n",
    "    alt.Y('y:Q'),\n",
    "    alt.Color('category:N')\n",
    ")\n",
    "\n",
    "base + base.transform_loess('Triage', 'y', groupby=['category']).mark_line(size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc83a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import altair as alt\n",
    "from vega_datasets import data\n",
    "\n",
    "source = PdmSEPSIS\n",
    "\n",
    "alt.Chart(source, title=\"The FFS Performance\").mark_point().encode(\n",
    "    alt.X(\n",
    "        'ProviderImpression:N',\n",
    "        title=\"ProviderImpression\",\n",
    "        scale=alt.Scale(zero=False),\n",
    "        axis=alt.Axis(grid=False)\n",
    "    ),\n",
    "    alt.Y(\n",
    "        'Triage:N',\n",
    "        title=\"\",\n",
    "        sort='-x',\n",
    "        axis=alt.Axis(grid=True)\n",
    "    ),\n",
    "    color=alt.Color('sepsis_outcome:N', legend=alt.Legend(title=\"sepsis_outcome\")),\n",
    "    row=alt.Row(\n",
    "        'SepsisStatus_Alert:N',\n",
    "        title=\"\",\n",
    "        sort=alt.EncodingSortField(field='ProviderImpression', op='sum', order='descending'),\n",
    "    )\n",
    ").properties(\n",
    "    height=alt.Step(20)\n",
    ").configure_view(stroke=\"transparent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b1106b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#How does Providor Impression affects the Outcome\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.scatter(x='ProviderImpression',y='sepsis_outcome',data=PdmSEPSIS)\n",
    "plt.xlabel('ProviderImpression')\n",
    "plt.ylabel('sepsis_outcome')\n",
    "\n",
    "# #How does Providor Impression affects the Triage\n",
    "# plt.figure(figsize=(10,10))\n",
    "# plt.scatter(x='ProviderImpression',y='Triage',data=PdmSEPSIS)\n",
    "# plt.xlabel('ProviderImpression')\n",
    "# plt.ylabel('Triage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb49603",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count Nulls\n",
    "print(PdmSEPSIS.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c32337d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#With the help of heatmap, we can see the amount of data that is missing from the attribute\n",
    "sns.heatmap(PdmSEPSIS.isnull(),cbar=False,cmap='viridis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e093d86",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# PdmSEPSIS.groupby(['AgeGroup_HIPAA','Accuracy'])['Accuracy'].count().unstack().plot(legend=True)\n",
    "# plt.title('AgeGroup_HIPAA and Accuracy')\n",
    "# plt.xlabel('AgeGroup_HIPAA')\n",
    "# plt.ylabel('Accuracy')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893c0e0b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Multi-panel Scatter Plot with Linked Brushing\n",
    "source = PdmSEPSIS\n",
    "\n",
    "selection = alt.selection_multi(fields=['Accuracy'], bind='legend')\n",
    "\n",
    "\n",
    "Accuracy = alt.Chart(source, title ='Accuracy of Primary Impressions among Age Groups and Gender', width=400, height=300).mark_circle().encode(\n",
    "    \n",
    "    x = 'AgeGroup_HIPAA:N',\n",
    "    y = alt.Y('count(AgeGroup_HIPAA):Q',title='Total Population'),   \n",
    "\n",
    "    color=alt.condition(brush, 'Accuracy:N', alt.value('lightgray')),      \n",
    " \n",
    "    tooltip=['PatientSex','Accuracy', 'AgeGroup_HIPAA', 'count(AgeGroup_HIPAA)'],\n",
    " \n",
    "    facet='PatientSex',\n",
    "      \n",
    "    size = alt.Size('count(AgeGroup_HIPAA)',title = 'Total Populaion',scale=alt.Scale(range=[100, 500]))\n",
    ").add_selection(\n",
    "    selection\n",
    ").add_selection(brush)\n",
    "Accuracy.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a9144e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Chart for reference\n",
    "\n",
    "source = PdmSEPSIS\n",
    "#.mark_line()/.mark_bar()/.mark_point()\n",
    "#scales = alt.selection_interval(bind='scales')  \n",
    "# interval = alt.selection_interval()\n",
    "# make_example(interval)\n",
    "Accuracy = alt.Chart(source, title ='Accuracy of Primary Impressions among Age Groups and Gender', width=400, height=300).mark_circle().encode(\n",
    "    \n",
    "    x = 'AgeGroup_HIPAA:N',\n",
    "    y = alt.Y('count(AgeGroup_HIPAA):Q',title='Total Population'),   \n",
    "#   alt.X('AgeGroup_HIPAA:N', axis=alt.Axis(domain=False, tickSize=0)),\n",
    "#   alt.Y('count(AgeGroup_HIPAA):Q',title='Total Population',stack='center', axis=None),\n",
    "    color=alt.condition(brush, 'Accuracy:N', alt.value('lightgray')),      \n",
    " \n",
    "    tooltip=['PatientSex','Accuracy', 'AgeGroup_HIPAA', 'count(AgeGroup_HIPAA)'],\n",
    "    #opacity=alt.condition(selection, alt.value(1), alt.value(0.2)),\n",
    "    facet='PatientSex',\n",
    "      \n",
    "    size = alt.Size('count(AgeGroup_HIPAA)',title = 'Total Populaion',scale=alt.Scale(range=[100, 500]))\n",
    "    ).add_selection(\n",
    "    selection\n",
    "    ).add_selection(brush).interactive()\n",
    "Accuracy.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be955ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import altair as alt\n",
    "from vega_datasets import data\n",
    "\n",
    "source = PdmSEPSIS\n",
    "selection = alt.selection_multi(fields=['Accuracy'], bind='legend')\n",
    "PatSEPSISChart = alt.Chart(source).mark_bar().encode(\n",
    "    x='AgeGroup_HIPAA:O',\n",
    "    y='count(AgeGroup_HIPAA):Q',\n",
    "    color='Triage:N',\n",
    "    column='SepsisStatus:N',\n",
    "    tooltip=['Triage', 'sepsis_outcome', 'AgeGroup_HIPAA', 'count(AgeGroup_HIPAA)']\n",
    ").add_selection(\n",
    "    selection\n",
    "    ).interactive()\n",
    "PatSEPSISChart .display()\n",
    "#PatDemChart.save('PatDemChart.html')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
